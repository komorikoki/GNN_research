# GNN を用いた分子 HOMO エネルギー予測

## 実験内容

本実験は, 分子特性予測タスクにおいて異なるメッセージパッシング機構を持つ代表的な 3 つのグラフニューラルネットワーク（GNN）モデル（GIN, PNA, GATv2）の性能（予測精度）と訓練安定性を比較評価する.

### 使用モデル

- GIN(Graph Isomorphism Network, https://arxiv.org/pdf/1810.00826): GIN はノード特徴量の集約に MLP を使用し, 集約後に自己特徴量を加算する機構を持った GNN である. 他の発展 GNN と比べ, メッセージの集約自体はシンプルな加算がベースであり, 計算効率が高く, 安定した訓練が可能である.
- PNA(Principal Neighbourhood Aggregation, https://arxiv.org/pdf/2004.05718): PNA は, 複数の集約関数を同時に用いて特徴量の集約を行うアーキテクチャである. 一般的には, 平均, 最大, 最小, 標準偏差などを組み合わせてノードの集約を行い, ノードの近傍における情報の分布全体を捉えることができる. これにより, 大きな次数を持つノードからのメッセージが支配的になりにくい. 複雑な特性を持つグラフの回帰タスクで有用である.
- GATv2(Graph Attention Network v2, https://arxiv.org/pdf/2105.14491): GATv2 は, GAT のアテンションスコアを計算する際に結合された特徴量を線形変換する順序を変更したアーキテクチャである. これにより, メッセージ送信元の位置によってアテンションスコアが異なる, 位置依存性をモデル化することが可能になった.

### 使用データセット

今回の実験では QM9 データセット(https://springernature.figshare.com/collections/Quantum_chemistry_structures_and_properties_of_134_kilo_molecules/978904/5)を使用する. 本データセットは最大 9 個の重原子を含む 13 万以上の安定な有機分子データを含んでおり, 入力 ($\mathbf{x}$)分子構造をグラフとして表現している. 各ノード ($\mathbf{x}$) は原子（C, O, N, F, H）であり, 特徴量は原子番号, 結合次数などのワンホットエンコーディングを用いている. エッジは原子間の化学結合である.

### 評価指標

本実験では評価指標として, MAE(Mean Absolute Error, 平均絶対誤差) [meV]を用いる. MAE は以下の式で与えられる.

$$MAE = \frac{1}{N} \sum_{i=1}^N |y_i - \hat{y_i}|$$

MAE は誤差を 2 乗する誤差を二乗する $\text{MSE}$ (Mean Squared Error) や $\text{RMSE}$ (Root Mean Squared Error) と比較して, 外れ値（極端に大きな誤差）の影響が過度に大きくならないという特徴がある.

## 実験詳細

本実験では, QM9 データセットの分子構造に含まれる各ノード特徴量とエッジ情報を用いて HOMO エネルギー (Highest Occupied Molecular Orbital energy)を予測する. HOMO エネルギーは分子の電子放出のしやすさを示す量子化学的特性であり, イオン化エネルギーに強い相関を持つ.

### 実験手順

1.  データ処理と設定データ分割: QM9 データセットを, 訓練セット, 検証セット, テストセットに約 8:1:1 の比率で分割する. データローダー: 各セットに対し, バッチサイズ 64 の DataLoader を作成し, ミニバッチ学習を可能にする. 共通ハイパーパラメータ: 隠れ層の次元を 64 に統一. 学習エポック数を 15 に設定する.
2.  モデルアーキテクチャの構築: GIN, PNA, GATv2 の 3 モデルを構築した. 各モデルのパラメータ数を以下に示す.
    | モデル名 | パラメータ数 | GNN 層 | グラフ集約層 | 予測層 |
    | :---: | ---: | :---: | :---: | :---: |
    | GIN | 13,313 | GINConv (3 層) | `global_add_pool` | Linear → Dropout → Linear |
    | PNA | 150,069 | PNAConv (3 層) | `global_add_pool` | Linear → Dropout → Linear |
    | GATv2 | 176,001 | GATv2Conv (3 層) | `global_add_pool` | Linear → Dropout → Linear |
3.  訓練と評価最適化: 学習には Adam オプティマイザ（学習率 $\text{lr}=0. 001$）と $\text{L}1\text{Loss}$ を使用する. 各エポック完了後, 検証データセットを用いて MAE を計算する. 訓練完了後, 最良のチェックポイントをロードしテストセットでの最終的な MAE を算出する.

## 実験結果

各エポックにおける学習データセットに対する MAE を以下に示す.
![TrainMAE](. /images/trainmae. png)
各エポックにおける検証データに対する MAE の最高値及び最終的なテストデータに対する MAE を以下に示す.
| モデル名 | 検証データ MAE(最高) | テストデータ MAE|
|:--------:|:----------:|:-----------:|
| GIN | 109. 28| 109. 00|
| PNA | 104. 02|103. 36|
| GATv2 | 237. 15|237. 07|

## 考察

### GIN の効率性と安定性について

GIN は最も少ないパラメータ数（13. 3k）とシンプルな集約機構にもかかわらず, PNA に匹敵する高性能（$109. 00 \text{ meV}$）を達成した. これは QM9 データセットが持つグラフ構造において, シンプルな加算集約がすでに構造識別の大部分を担えるほど効率的であることを示している.

### PNA の最高性能と不安定性について

PNA が最も低い MAE（$103. 36 \text{ meV}$）を達成したのは, その多重集約機構に起因する. Max/Min/Std といった集約関数が, 単純な加算集約（GIN）では捉えられない, 分子構造の局所的な分散や外れ値的な結合パターンといった情報を効率的に抽出できたと考えられる. PNA の多重集約関数, 特に非線形性の強い Max/Min 集約が, ミニバッチ内のデータ変動や外れ値に過剰に反応し, 大きな重み更新ステップを引き起こした. その結果, 学習率が最適でも損失曲面上で最適な解を飛び越え, 検証 MAE が大きく変動する不安定性が生じたことが考えられる.

### GATv2 の低性能と過学習の可能性過学習の可能性

GATv2 は最大のパラメータ数（90. 8k）を持ちながら, MAE が $237. 00 \text{ meV}$ と他モデルと比べて低い. これは, モデルの複雑性がタスクの必要性に対して過剰であり, 訓練データに存在するノイズや詳細なパターンに過度に適合し, 汎化能力が低下した（過学習）可能性が高い. 分子グラフのような低次数かつ小さなグラフでは, 複雑な Multi-Head Attention を使用しても, シンプルな GIN の集約を超えるほどの効果的なノード間重み付けが学習できず, かえって計算コストとパラメータの増加を招いたと考えられる.

## 結論

本実験は, GNN モデルの選択において単純な「複雑性（パラメータ数）」ではなく, 集約メカニズムの特性がタスク性能と訓練の安定性に強く影響することを証明した. 特に, GIN は QM9 データセットにおいて少ないパラメータ数で高い予測性能を発揮する可能性を秘めていることが示された. また, PNA は最適なパラメータ調整を行う必要があることも示唆された.
